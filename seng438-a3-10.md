**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report #3 – Code Coverage, Adequacy Criteria and Test Case Correlation**

| Group: 10      |
|-----------------|
|J. Ty|   
|Kairos|   
|Luis|   
|Sayma|

# Table of Contents
1. [Introduction](#introduction)
2. [Manual data-flow coverage calculations for X and Y methods](#par1)
3. [A detailed description of the testing strategy for the new unit test](#par2)
4. [A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage](#par3)
5. [A detailed report of the coverage achieved of each class and method](#par4)
6. [Pros and Cons of coverage tools used and Metrics you report](#par5)
7. [A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation.](#par6)
8. [A discussion on how the team work/effort was divided and managed](#par7)
9. [Any difficulties encountered, challenges overcome, and lessons learned from performing the lab](#par8)
10. [Comments/feedback on the lab itself](#par9)


# Introduction <a name="introduction"></a>
In this lab session, we were able to learn to use a different technique, namely the white-box coverage criteria to decide what test cases we should develop. This lab session builds on the previous assignment, where our test suites are not scrutinized based on various coverage criterias. This coverage information can enable us to write better test suites.


# Manual data-flow coverage calculations for calculateColumnTotal and Y methods <a name="par1"></a>
Below is the manual data-flow coverage for the *Data Utilities calculateColumnTotal* method.

**Figure 1 - calculateColumnTotal Data Flow Graph**
![Alt text](/media/CCTGraph.png?raw=true "Figure 1 - CCT Data Flow Graph")

**Figure 2 - calculateColumnTotal Def-Use Sets and DU Pair Data**
![Alt text](/media/CCTHandDone.png?raw=true "Figure 2 - CCT DU Pair")

Below is the manual data-flow coverage for the *Range getLength* method.

**Figure 3 - getLength Data Flow Graph**
![Alt text](/media/GLGraph.png?raw=true "Figure 3 - GL Data Flow Graph")

**Figure 2 - getLength Def-Use Sets and DU Pair Data**
![Alt text](/media/GLHandDone.png?raw=true "Figure 4 - GL DU Pair")

# A detailed description of the testing strategy for the new unit test <a name="par2"></a>
All ten of our test case methods met the branch, statement and method coverage requirements needed for this lab. Originally the lowest coverage that any of our test cases had was the method coverage of *DataUtilitiesCreateNumberArray*, with a 88.9%. Because of this, the only way in which we were able to up our coverages for all three coverage tests was to remove all of the testing methods in which an exception was the expected outcome, as they do not end up running fully and thus do not cover anything. After the removal of these methods, all 10 of our test cases have 100% coverage for all of three coverages. JUnit 4 has no exception test coverage, therefore these methods cannot be tested for their coverage in our current system and have been commented out within our test case files. However it should be noted that coverage improved when try/catch blocks were added, not because of the coverage but because of the number of lines that were covered. The original exception tests were still not covered so the try/catch blocks simply acted as padding.



# A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage <a name="par3"></a>
Using our knowledge of coverage, we have edited five of our test cases in order to increase their coverages. The tests included: 
*DataUtilitiesCreateNumberArray*
*DataUtilitiesCreateNumberArray2d*
*DataUtilitiesCalculateColumnTotal*
*RangeGetUpperBound*
*RangeGetLowerBound*
Specifically the methods:
createNumberArrayTestNull() in *DataUtilitiesCreateNumberArray*
createNumberArray2DTestNull() in *DataUtilitiesCreateNumberArray2d*
givenPositiveValuedRowsTableButNegativeIndex_whenCalculateColumnTotal_thenReturnZero() in *DataUtilitiesCalculateColumnTotal*
givenPositiveValuedRowsTableButWrongIndex_whenCalculateColumnTotal_thenReturnZero() in *DataUtilitiesCalculateColumnTotal*
upperBoundOfInvalidInputs() in *RangeGetUpperBound*
lowerBoundOfInvalidInputs() in *RangeGetLowerBound*
With these tests that left room for improvement, we tried different ways to ensure all the lines were covered. However, we noticed that all statements that weren’t covered belonged to tests that involved exception handling. As such, we set out to find a way to cover exception tests. Through some research, we learned that JUnit 4 does not offer exception coverage, and therefore could not be improved upon besides adding padding to increase the coverage percentage. We did however, add try/catch blocks to the creatNumberArrayTestNull functions, and that improved the function coverage by 60%, and class coverage by 4%. We ended up commenting out all of the methods that were not covered due to throwing exceptions, and thus increased the code coverage as it no longer needed to be covered.



# A detailed report of the coverage achieved of each class and method <a name="par4"></a>
Below are figures showing the three coverages that we tested.

**Figure 5 - Statement Coverage**
![Alt text](/media/statementcoverage.png?raw=true "Figure 5 - Statement Coverage")


**Figure 6 - Branch Coverage**
![Alt text](/media/branchcoverage.png?raw=true "Figure 6 - Branch Coverage")
For branch coverage, only one of our ten methods included a branch, thus it is the only method to report a coverage, as the others have nothing to cover.


**Figure 7 - Method Coverage**
![Alt text](/media/methodcoverage.png?raw=true "Figure 7 - Method Coverage")


# Pros and Cons of coverage tools used and Metrics you report <a name="par5"></a>
The coverage tool that our team decided to go with is EclEmma. The metrics that we are using are statement, branch and method.
Some of the pros and cons are…




# A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation. <a name="par6"></a>
Some advantages of requirements-based test generation is that it is effective in testing based on what the actual requirements of the software needs as you can easily check if the input can produce the desired output. Requirements-based testing can also give you traceability between requirements and test cases which can be useful for checking how the code complies based on the requirements that was set forth. Requirements-based testing though provides a limitation which is that it may not catch issues that can come up from unexpected interactions between software like if statements or other conditions in the source code that might no longer be accessed if you tested purely based on the requirements. Coverage-based testing however is more focused on the workings of the source code itself and the flow of the code itself. This makes it disadvantageous though as this method of testing can mean that the user can miss out on some use cases that are part of the requirements for the program.




# A discussion on how the team work/effort was divided and managed <a name="par7"></a>
All tasks were completed as a group effort during meetings. We started with a design discussion session where we allocated the work and chose the tool and metrics to use. All work was split up, discussed, and revised by other group members. We started with discussing our previous test cases and looked at the coverage. Once we had an idea about what tests needed to be edited, we split them equally amongst ourselves and worked on them. After, we discussed the changes we made and why, and how it affected the coverage percentage. We then worked on our parts adjacently, and cross-checked our portions during lab sessions and through online meetings. The allocation of test suites to improve the coverage was based on the previous assignment, since we are more familiar with the test suites that we wrote.




# Any difficulties encountered, challenges overcome, and lessons learned from performing the lab <a name="par8"></a>
One of the main difficulties was increasing the coverage percentage to the desired outcome. This was especially difficult since we had to have a deep understanding of the underlying code, which was not written by ourselves. However, this serves as a good practice since we might have to write test suites for codes developed by other engineers.
One lesson that we learnt was to use tools like EclEmma to determine the coverage percentage of our test suites. This was extremely useful since we were able to definitively put a number to how good our test suites are.




# Comments/feedback on the lab itself <a name="par9"></a>
Overall, our team improved our teamwork and deepened our knowledge on extracting coverage metrics from the tool EclEmma. This lab was very insightful and served as a good practice which can help us in our future endeavors in software engineering.

