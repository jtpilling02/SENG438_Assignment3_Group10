**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report #3 – Code Coverage, Adequacy Criteria and Test Case Correlation**

| Group: 10      |
|-----------------|
|J. Ty|   
|Kairos|   
|Luis|   
|Sayma|

# Table of Contents
1. [Introduction](#introduction)
2. [Manual data-flow coverage calculations for X and Y methods](#par1)
3. [A detailed description of the testing strategy for the new unit test](#par2)
4. [A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage](#par3)
5. [A detailed report of the coverage achieved of each class and method](#par4)
6. [Pros and Cons of coverage tools used and Metrics you report](#par5)
7. [A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation.](#par6)
8. [A discussion on how the team work/effort was divided and managed](#par7)
9. [Any difficulties encountered, challenges overcome, and lessons learned from performing the lab](#par8)
10. [Comments/feedback on the lab itself](#par9)


# Introduction <a name="introduction"></a>
In this lab session, we were able to learn to use a different technique, namely the white-box coverage criteria to decide what test cases we should develop. This lab session builds on the previous assignment, where our test suites are not scrutinized based on various coverage criterias. This coverage information can enable us to write better test suites.


# Manual data-flow coverage calculations for X and Y methods <a name="par1"></a>
Text…


# A detailed description of the testing strategy for the new unit test <a name="par2"></a>
All ten of our test case methods met the branch, statement and method coverage requirements needed for this lab. The lowest coverage that any of our test cases had was the method coverage of *DataUtilitiesCreateNumberArray*, with a 88.9%. Because of this we found no need to update any of our test cases as they sufficiently met the criterias. 


# A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage <a name="par3"></a>
Text…


# A detailed report of the coverage achieved of each class and method <a name="par4"></a>
Below are figures showing the three coverages that we tested.

**Figure 1 - Statement Coverage**
![Alt text](/media/statementcoverage.png?raw=true "Figure 1 - Statement Coverage")


**Figure 2 - Branch Coverage**
![Alt text](/media/branchcoverage.png?raw=true "Figure 2 - Branch Coverage")
For branch coverage, only one of our ten methods included a branch, thus it is the only method to report a coverage, as the others have nothing to cover.


**Figure 3 - Method Coverage**
![Alt text](/media/methodcoverage.png?raw=true "Figure 3 - Method Coverage")


# Pros and Cons of coverage tools used and Metrics you report <a name="par5"></a>
The coverage tool that our team decided to go with is EclEmma. The metrics that we are using are statement, branch and method.
Some of the pros and cons are…


# A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation. <a name="par6"></a>
Text…


# A discussion on how the team work/effort was divided and managed <a name="par7"></a>
Every single task was divided almost equally amongst the group members. We started with a design discussion session where we allocated the work and chose the tool and metrics to use. We then worked on our parts adjacently, and cross-checked our portions during lab sessions and through online meetings. The allocation of test suites to improve the coverage was based on the previous assignment, since we are more familiar with the test suites that we wrote.


# Any difficulties encountered, challenges overcome, and lessons learned from performing the lab <a name="par8"></a>
One of the main difficulties was increasing the coverage percentage to the desired outcome. This was especially difficult since we had to have a deep understanding of the underlying code, which was not written by ourselves. However, this serves as a good practice since we might have to write test suites for codes developed by other engineers.
One lesson that we learnt was to use tools like EclEmma to determine the coverage percentage of our test suites. This was extremely useful since we were able to definitively put a number to how good our test suites are.


# Comments/feedback on the lab itself <a name="par9"></a>
Overall, our team improved our teamwork and deepened our knowledge on extracting coverage metrics from the tool EclEmma. This lab was very insightful and served as a good practice which can help us in our future endeavors in software engineering.

