**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report #3 – Code Coverage, Adequacy Criteria and Test Case Correlation**

| Group: 10      |
|-----------------|
|J. Ty|   
|Kairos|   
|Luis|   
|Sayma|

# Table of Contents
1. [Introduction](#introduction)
2. [Manual data-flow coverage calculations for X and Y methods](#par1)
3. [A detailed description of the testing strategy for the new unit test](#par2)
4. [A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage](#par3)
5. [A detailed report of the coverage achieved of each class and method](#par4)
6. [Pros and Cons of coverage tools used and Metrics you report](#par5)
7. [A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation.](#par6)
8. [A discussion on how the team work/effort was divided and managed](#par7)
9. [Any difficulties encountered, challenges overcome, and lessons learned from performing the lab](#par8)
10. [Comments/feedback on the lab itself](#par9)

# Introduction <a name="introduction"></a>
In this lab session, we were able to learn to use a different technique, namely the white-box coverage criteria to decide what test cases we should develop. This lab session builds on the previous assignment, where our test suites are not scrutinized based on various coverage criterias. This coverage information can enable us to write better test suites.

# Manual data-flow coverage calculations for calculateColumnTotal and getLength methods <a name="par1"></a>
Below is the manual data-flow coverage for the *Data Utilities calculateColumnTotal* method.

**Figure 1 - calculateColumnTotal Data Flow Graph**
![Alt text](/media/CCTGraph.png?raw=true "Figure 1 - CCT Data Flow Graph")

**Figure 2 - calculateColumnTotal Def-Use Sets and DU Pair Data**
![Alt text](/media/CCTHandDone.png?raw=true "Figure 2 - CCT DU Pair")

Below is the manual data-flow coverage for the *Range getLength* method.

**Figure 3 - getLength Data Flow Graph**
![Alt text](/media/GLGraph.png?raw=true "Figure 3 - GL Data Flow Graph")

**Figure 2 - getLength Def-Use Sets and DU Pair Data**
![Alt text](/media/GLHandDone.png?raw=true "Figure 4 - GL DU Pair")

# A detailed description of the testing strategy for the new unit test <a name="par2"></a>

In order to create the new test cases needed in order to reach the branch, statement, and method coverages to the required amount, we had to develop test cases for all of the methods that we did not write beforehand. To do this, we did a very similar format to the second lab, by finding out the partitions that we needed to cover in order to satisfy the coverages in *Range* and *DataUtilities*. The main difference between our strategy from this lab compared to the last, is that instead of using the requirements that each method needed, we were able to look at the source code itself, to find out each branch that needed to have a partition written for it.

# A high level description of five selected test cases you have designed using coverage information, and how they have increased code coverage <a name="par3"></a>
Using our knowledge of coverage, we have edited five of our test cases in order to increase their coverages. The tests included:

*DataUtilitiesCreateNumberArray*

*DataUtilitiesCreateNumberArray2d*

*DataUtilitiesCalculateColumnTotal*

*RangeGetUpperBound*

*RangeGetLowerBound*

Specifically the methods:

createNumberArrayTestNull() in *DataUtilitiesCreateNumberArray*

createNumberArray2DTestNull() in *DataUtilitiesCreateNumberArray2d*

givenPositiveValuedRowsTableButNegativeIndex_whenCalculateColumnTotal_thenReturnZero() in *DataUtilitiesCalculateColumnTotal*

givenPositiveValuedRowsTableButWrongIndex_whenCalculateColumnTotal_thenReturnZero() in *DataUtilitiesCalculateColumnTotal*

upperBoundOfInvalidInputs() in *RangeGetUpperBound*

lowerBoundOfInvalidInputs() in *RangeGetLowerBound*

With these tests that left room for improvement, we tried different ways to ensure all the lines were covered. However, we noticed that all statements that weren’t covered belonged to tests that involved exception handling. As such, we set out to find a way to cover exception tests. Through some research, we learned that JUnit 4 does not offer exception coverage, and therefore could not be improved upon besides adding padding to increase the coverage percentage. We did however, add try/catch blocks to the creatNumberArrayTestNull functions, and that improved the function coverage by 60%, and class coverage by 4%. We ended up commenting out all of the methods that were not covered due to throwing exceptions, and thus increased the code coverage as it no longer needed to be covered.

# A detailed report of the coverage achieved of each class and method <a name="par4"></a>
Below are figures showing the three coverages that we tested. These three coverages were Statement, Branch, and Method coverages. Due to using EclEmma, we substituted condition coverage for method coverage, as that was what was available for us to use.

Our branch and statement coverages ended up being lower for certain methods due to the inability to test some of the branches that should cover. The most likely cause for this error was due to the branch being unable to be tested, as the input values needed caused an exception that JUnit 4 is unable to handle. This was most commonly seen throughout *Range* when the *'if'* statement had a branch in which the lower bound needed to be larger than the upper bound. This is invalid within the constructor of *Range*, thus could not be instantiated and therefore could not be tested. This led to our branch and statement coverages being lower or very close to the required percentages.

**Figure 1 - Statement Coverage (A.K.A. Line Coverage)**
![Alt text](/media/statementcoverage.png?raw=true "Figure 1 - Statement Coverage")

**Figure 2 - Branch Coverage**
![Alt text](/media/branchcoverage.png?raw=true "Figure 2 - Branch Coverage")

**Figure 3 - Method Coverage**
![Alt text](/media/methodcoverage.png?raw=true "Figure 3 - Method Coverage")

# Pros and Cons of coverage tools used and Metrics you report <a name="par5"></a>
The coverage tool that our team decided to go with is EclEmma. The metrics that we are using are statement, branch and method. Some pros for using EclEmma is that it provides an accurate measure of code coverage. In this lab, the group was easily able to check how much of the metrics chosen were covered with each time the testing suite that the group developed was ran. However, a consequence of using coverage tools such as this is that it can be a crutch for developers. Code coverage, while it can be good, is not the only measure of test quality. 

When looking at the three coverage methods that we used, each of them had their own advantages and disadvantages. Firstly, branch coverage covers every branch in a method atleast once. A branch in this case is a statement that has two or more paths, such as an *if* or a *for* statement. Branch coverages advantages are that it validates that every branch of the code is reached, along with that no branches lead to abnormal functionalities. The main drawback of this metric is that it can sometimes ignore code segments, such as those within a boolean branch.

The next metric is statement coverage. This metric checks for each executed statements within a method. Some advantages of this method is the ability to verify all of the code works as intended. This in turn helps measure the quality of the code as a whole. It will also check all of the branches within a method. The major drawbacks of this metric is it's inability to test false conditions, and that it has difficulty understanding logic operators. This is the most complex metric to implement due to the vast coverage it should provide.

The final metric used is method coverage. This metric checks if a method has been accessed at all throughout execution. This is a fairly simple metric, and it's main selling point is the quick knowledge on if a method has been tested. Because it only tests if a method is accessed, it had a major flaw in that it does not indicate whether a method fully functions. This is often the easiest metric to implement.

All three of these coverages are fairly straight forward and easy to use and implement with proper understanding, but had some issues with JUnit 4, as functionality of asserting exceptions was not introduced until JUnit 5, and thus was not usable for this lab.

# A comparison on the advantages and disadvantages of requirements-based test generation and coverage-based test generation. <a name="par6"></a>
Some advantages of requirements-based test generation is that it is effective in testing based on what the actual requirements of the software needs as you can easily check if the input can produce the desired output. Requirements-based testing can also give you traceability between requirements and test cases which can be useful for checking how the code complies based on the requirements that was set forth. Requirements-based testing though provides a limitation which is that it may not catch issues that can come up from unexpected interactions between software like if statements or other conditions in the source code that might no longer be accessed if you tested purely based on the requirements. Coverage-based testing however is more focused on the workings of the source code itself and the flow of the code itself. This makes it disadvantageous though as this method of testing can mean that the user can miss out on some use cases that are part of the requirements for the program.

# A discussion on how the team work/effort was divided and managed <a name="par7"></a>
All tasks were completed as a group effort during meetings. We started with a design discussion session where we allocated the work and chose the tool and metrics to use. All work was split up, discussed, and revised by other group members. We started with discussing our previous test cases and looked at the coverage. Once we had an idea about what tests needed to be edited, we split them equally amongst ourselves and worked on them. After, we discussed the changes we made and why, and how it affected the coverage percentage. We then worked on our parts adjacently, and cross-checked our portions during lab sessions and through online meetings. The allocation of test suites to improve the coverage was based on the previous assignment, since we are more familiar with the test suites that we wrote.

# Any difficulties encountered, challenges overcome, and lessons learned from performing the lab <a name="par8"></a>
One of the main difficulties was increasing the coverage percentage to the desired outcome. This was especially difficult since we had to have a deep understanding of the underlying code, which was not written by ourselves. However, this serves as a good practice since we might have to write test suites for codes developed by other engineers. The other major difficulty was just figuring out which methods of the source code we needed to write new test cases for, as we did not have them for the last lab.
One lesson that we learnt was to use tools like EclEmma to determine the coverage percentage of our test suites. This was extremely useful since we were able to definitively put a number to how good our test suites are.

# Comments/feedback on the lab itself <a name="par9"></a>
Overall, our team improved our teamwork and deepened our knowledge on extracting coverage metrics from the tool EclEmma. This lab was very insightful and served as a good practice which can help us in our future endeavors in software engineering.
